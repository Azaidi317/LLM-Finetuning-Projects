{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Azaidi317/LLM-Finetuning-Projects/blob/main/stock_sentiment_using_Microsoft_phi2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iyg5bNkEGC_x",
        "outputId": "35ec01f7-1307-418e-ea23-8ea4c79bcc3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting prawcore<3,>=2.4 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update_checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.8.30)\n",
            "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: update_checker, prawcore, praw\n",
            "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n"
          ]
        }
      ],
      "source": [
        "pip install praw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "953e0055f1fc4445b17b6d8e3b4dbc73",
            "032a243243734e7e9be9fcb72dad8cd3",
            "b5056af7e0e14cbc9ce9ca7808d1902f",
            "126d9196623b4f8d88db587298a3aa5f",
            "aa0ca269338c46d09b7fb3f278a29803",
            "14c7c1087eaf490b91f244672c8a072c",
            "37e30a3be4e34d589d195af403be5001",
            "0bfe9537393346829f05c6feb7d26b2d",
            "1c07ea446a684a5d9f04b75e6381cb22",
            "d05b251602184a2e8d11ce4379ad58d2",
            "93e4a07302fd4247b07216d94287b836"
          ]
        },
        "id": "2w-03_ApFJ8N",
        "outputId": "62e347c8-0406-41c2-c96c-1104b3bebb4c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "953e0055f1fc4445b17b6d8e3b4dbc73",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of PhiForSequenceClassification were not initialized from the model checkpoint at microsoft/phi-2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analyzing posts about NVDA from r/wallstreetbets...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]<ipython-input-5-f2bb68e45c0f>:125: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  stock_data['Close'][-1] / stock_data['Close'][0] - 1\n",
            "50it [00:32,  1.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Analysis Results:\n",
            "Overall Sentiment Score: 0.98\n",
            "\n",
            "Sentiment Distribution:\n",
            "sentiment\n",
            "Strong Bullish    0.88\n",
            "Bearish           0.10\n",
            "Strong Bearish    0.02\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Most Impactful Posts:\n",
            "\n",
            "Title: YOLO $35k into 1.1MM to save the family farm. \n",
            "Sentiment: Strong Bullish (Score: 2.00)\n",
            "Confidence: 0.65\n",
            "Post Score: 12004\n",
            "\n",
            "Title: I ded it agan\n",
            "Sentiment: Strong Bullish (Score: 2.00)\n",
            "Confidence: 0.40\n",
            "Post Score: 3805\n",
            "\n",
            "Title: Started taking NVidia gains after 12 years\n",
            "Sentiment: Strong Bullish (Score: 2.00)\n",
            "Confidence: 0.44\n",
            "Post Score: 3003\n",
            "\n",
            "Title: Mega Bull is about to be unleashed\n",
            "Sentiment: Strong Bullish (Score: 2.00)\n",
            "Confidence: 0.34\n",
            "Post Score: 2708\n",
            "\n",
            "Title: I shorted MSTR; no such thing as infinite money glitch \n",
            "Sentiment: Strong Bullish (Score: 2.00)\n",
            "Confidence: 0.49\n",
            "Post Score: 1074\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import praw\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import yfinance as yf\n",
        "\n",
        "class AdvancedStockAnalyzer:\n",
        "    def __init__(self, client_id, client_secret, user_agent):\n",
        "        # Initialize Reddit API\n",
        "        self.reddit = praw.Reddit(\n",
        "            client_id='Nr1OwEqV_a8GVY3_jxU9-w',\n",
        "            client_secret='sO0micrWKauECqX7bkR4ztxtLxtsEA',\n",
        "            user_agent='stock_bot/1.0 by Same_Can_7313',\n",
        "            check_for_async=False\n",
        "        )\n",
        "\n",
        "        # Initialize advanced sentiment analyzer\n",
        "        # Using BloombergGPT fine-tuned for financial analysis\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            \"microsoft/phi-2\",\n",
        "            num_labels=5,  # Strong Bearish, Bearish, Neutral, Bullish, Strong Bullish\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # Move model to GPU if available\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Custom prompts for better analysis\n",
        "        self.prompt_template = \"\"\"\n",
        "        Analyze the following text about {stock_symbol} stock and determine the sentiment:\n",
        "\n",
        "        Text: {text}\n",
        "\n",
        "        Consider the following factors:\n",
        "        1. Overall market sentiment\n",
        "        2. Financial metrics mentioned\n",
        "        3. Technical analysis indicators\n",
        "        4. Company-specific news\n",
        "        5. Industry trends\n",
        "\n",
        "        Classify the sentiment as one of:\n",
        "        - Strong Bearish (-2)\n",
        "        - Bearish (-1)\n",
        "        - Neutral (0)\n",
        "        - Bullish (1)\n",
        "        - Strong Bullish (2)\n",
        "\n",
        "        Also extract key points that support this sentiment.\n",
        "        \"\"\"\n",
        "\n",
        "    def get_stock_data(self, symbol, period=\"1mo\"):\n",
        "        \"\"\"Get recent stock data for context\"\"\"\n",
        "        try:\n",
        "            stock = yf.Ticker(symbol)\n",
        "            history = stock.history(period=period)\n",
        "            return history\n",
        "        except Exception as e:\n",
        "            print(f\"Error getting stock data: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def analyze_text_sentiment(self, text, stock_symbol):\n",
        "        \"\"\"Advanced sentiment analysis with context\"\"\"\n",
        "        try:\n",
        "            # Format prompt with context\n",
        "            formatted_prompt = self.prompt_template.format(\n",
        "                stock_symbol=stock_symbol,\n",
        "                text=text\n",
        "            )\n",
        "\n",
        "            # Tokenize and analyze\n",
        "            inputs = self.tokenizer(\n",
        "                formatted_prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=512\n",
        "            ).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "            # Map predictions to sentiments\n",
        "            sentiment_map = {\n",
        "                0: \"Strong Bearish\",\n",
        "                1: \"Bearish\",\n",
        "                2: \"Neutral\",\n",
        "                3: \"Bullish\",\n",
        "                4: \"Strong Bullish\"\n",
        "            }\n",
        "\n",
        "            sentiment_idx = torch.argmax(predictions).item()\n",
        "            confidence = predictions[0][sentiment_idx].item()\n",
        "\n",
        "            # Calculate sentiment score (-2 to 2)\n",
        "            sentiment_score = sentiment_idx - 2\n",
        "\n",
        "            return {\n",
        "                'label': sentiment_map[sentiment_idx],\n",
        "                'score': sentiment_score,\n",
        "                'confidence': confidence\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error analyzing sentiment: {str(e)}\")\n",
        "            return {\n",
        "                'label': \"Neutral\",\n",
        "                'score': 0,\n",
        "                'confidence': 0\n",
        "            }\n",
        "\n",
        "    def analyze_with_market_context(self, text, stock_symbol, stock_data=None):\n",
        "        \"\"\"Analyze sentiment with market context\"\"\"\n",
        "        # Get basic sentiment\n",
        "        base_sentiment = self.analyze_text_sentiment(text, stock_symbol)\n",
        "\n",
        "        if stock_data is not None:\n",
        "            # Calculate recent price trend\n",
        "            recent_return = (\n",
        "                stock_data['Close'][-1] / stock_data['Close'][0] - 1\n",
        "            ) * 100\n",
        "\n",
        "            # Adjust sentiment based on market context\n",
        "            if abs(recent_return) > 10:  # Significant price movement\n",
        "                if recent_return > 0:\n",
        "                    base_sentiment['score'] *= 1.2  # Amplify positive sentiment\n",
        "                else:\n",
        "                    base_sentiment['score'] *= 0.8  # Dampen negative sentiment\n",
        "\n",
        "        return base_sentiment\n",
        "\n",
        "    def collect_and_analyze(self, subreddit_name=\"wallstreetbets\",\n",
        "                          stock_symbol=\"NVDA\", limit=100):\n",
        "        \"\"\"Collect posts and perform advanced analysis\"\"\"\n",
        "        try:\n",
        "            # Get stock data for context\n",
        "            stock_data = self.get_stock_data(stock_symbol)\n",
        "\n",
        "            # Access subreddit\n",
        "            subreddit = self.reddit.subreddit(subreddit_name)\n",
        "            posts_data = []\n",
        "\n",
        "            print(f\"Analyzing posts about {stock_symbol} from r/{subreddit_name}...\")\n",
        "\n",
        "            for post in tqdm(subreddit.search(stock_symbol, limit=limit, sort='new')):\n",
        "                # Combine title and text\n",
        "                full_text = f\"{post.title} {post.selftext}\"\n",
        "\n",
        "                # Analyze sentiment with market context\n",
        "                sentiment = self.analyze_with_market_context(\n",
        "                    full_text,\n",
        "                    stock_symbol,\n",
        "                    stock_data\n",
        "                )\n",
        "\n",
        "                post_data = {\n",
        "                    'title': post.title,\n",
        "                    'text': post.selftext,\n",
        "                    'score': post.score,\n",
        "                    'created_utc': datetime.fromtimestamp(post.created_utc),\n",
        "                    'num_comments': post.num_comments,\n",
        "                    'url': f\"https://reddit.com{post.permalink}\",\n",
        "                    'sentiment': sentiment['label'],\n",
        "                    'sentiment_score': sentiment['score'],\n",
        "                    'confidence': sentiment['confidence']\n",
        "                }\n",
        "                posts_data.append(post_data)\n",
        "                time.sleep(0.5)\n",
        "\n",
        "            # Create DataFrame\n",
        "            df = pd.DataFrame(posts_data)\n",
        "\n",
        "            # Calculate weighted sentiment score\n",
        "            df['weighted_sentiment'] = df['sentiment_score'] * df['score'] * df['confidence']\n",
        "            overall_sentiment = df['weighted_sentiment'].sum() / df['score'].sum()\n",
        "\n",
        "            # Save results\n",
        "            filename = f\"{stock_symbol}_advanced_sentiment.csv\"\n",
        "            df.to_csv(filename, index=False)\n",
        "\n",
        "            return df, overall_sentiment\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in analysis: {str(e)}\")\n",
        "            return None, None\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize analyzer\n",
        "    analyzer = AdvancedStockAnalyzer(\n",
        "        client_id=\"your_client_id\",\n",
        "        client_secret=\"your_client_secret\",\n",
        "        user_agent=\"AdvancedStockBot/1.0\"\n",
        "    )\n",
        "\n",
        "    # Analyze stock\n",
        "    df, overall_sentiment = analyzer.collect_and_analyze(\n",
        "        subreddit_name=\"wallstreetbets\",\n",
        "        stock_symbol=\"NVDA\",\n",
        "        limit=50\n",
        "    )\n",
        "\n",
        "    if df is not None:\n",
        "        print(\"\\nAnalysis Results:\")\n",
        "        print(f\"Overall Sentiment Score: {overall_sentiment:.2f}\")\n",
        "        print(\"\\nSentiment Distribution:\")\n",
        "        print(df['sentiment'].value_counts(normalize=True))\n",
        "\n",
        "        print(\"\\nMost Impactful Posts:\")\n",
        "        impact_posts = df.nlargest(5, 'weighted_sentiment')\n",
        "        for _, post in impact_posts.iterrows():\n",
        "            print(f\"\\nTitle: {post['title']}\")\n",
        "            print(f\"Sentiment: {post['sentiment']} (Score: {post['sentiment_score']:.2f})\")\n",
        "            print(f\"Confidence: {post['confidence']:.2f}\")\n",
        "            print(f\"Post Score: {post['score']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code to read it from all subreddits, not just one!"
      ],
      "metadata": {
        "id": "3Ll_O-VwXVIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "class RedditStockAnalyzer:\n",
        "    def __init__(self, client_id, client_secret, user_agent):\n",
        "        # Initialize Reddit API\n",
        "        self.reddit = praw.Reddit(\n",
        "            client_id='Nr1OwEqV_a8GVY3_jxU9-w',\n",
        "            client_secret='sO0micrWKauECqX7bkR4ztxtLxtsEA',\n",
        "            user_agent='stock_bot/1.0 by Same_Can_7313',\n",
        "            check_for_async=False\n",
        "        )\n",
        "\n",
        "        # Initialize sentiment analyzer\n",
        "        self.sentiment_analyzer = pipeline(\n",
        "            \"sentiment-analysis\",\n",
        "            model=\"ProsusAI/finbert\"\n",
        "        )\n",
        "\n",
        "        # Popular finance-related subreddits to check individually\n",
        "        self.finance_subreddits = [\n",
        "            \"wallstreetbets\", \"stocks\", \"investing\", \"stockmarket\",\n",
        "            \"options\", \"pennystocks\", \"cryptocurrency\", \"finance\",\n",
        "            \"business\", \"SecurityAnalysis\", \"algotrading\"\n",
        "        ]\n",
        "\n",
        "    def analyze_text_sentiment(self, text):\n",
        "        \"\"\"Analyze sentiment of text\"\"\"\n",
        "        try:\n",
        "            if not text or pd.isna(text):\n",
        "                return {'label': 'neutral', 'score': 0.0}\n",
        "            result = self.sentiment_analyzer(text[:512])[0]\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(f\"Sentiment analysis error: {str(e)}\")\n",
        "            return {'label': 'neutral', 'score': 0.0}\n",
        "\n",
        "    def search_all_reddit(self, stock_symbol, time_filter='week', limit=100):\n",
        "        \"\"\"Search all of Reddit for stock mentions\"\"\"\n",
        "        all_posts = []\n",
        "\n",
        "        try:\n",
        "            print(f\"Searching all Reddit for {stock_symbol}...\")\n",
        "\n",
        "            # Search r/all\n",
        "            for submission in tqdm(self.reddit.subreddit(\"all\").search(\n",
        "                f'\"{stock_symbol}\"', # Exact match search\n",
        "                sort='hot',\n",
        "                time_filter=time_filter,\n",
        "                limit=limit\n",
        "            )):\n",
        "                # Basic filtering to avoid false positives\n",
        "                if self._is_valid_stock_mention(submission.title + \" \" + submission.selftext, stock_symbol):\n",
        "                    post_data = self._extract_post_data(submission, stock_symbol, \"r/all\")\n",
        "                    all_posts.append(post_data)\n",
        "\n",
        "            # Search specific finance subreddits\n",
        "            for subreddit in self.finance_subreddits:\n",
        "                try:\n",
        "                    print(f\"\\nSearching r/{subreddit}...\")\n",
        "                    subreddit_posts = self.reddit.subreddit(subreddit).search(\n",
        "                        f'\"{stock_symbol}\"',\n",
        "                        sort='hot',\n",
        "                        time_filter=time_filter,\n",
        "                        limit=int(limit/2)  # Smaller limit for individual subreddits\n",
        "                    )\n",
        "\n",
        "                    for submission in subreddit_posts:\n",
        "                        if self._is_valid_stock_mention(submission.title + \" \" + submission.selftext, stock_symbol):\n",
        "                            post_data = self._extract_post_data(submission, stock_symbol, subreddit)\n",
        "                            all_posts.append(post_data)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error searching r/{subreddit}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "                time.sleep(0.5)  # Respect rate limits\n",
        "\n",
        "            # Convert to DataFrame and remove duplicates\n",
        "            df = pd.DataFrame(all_posts)\n",
        "            df = df.drop_duplicates(subset=['id'])\n",
        "\n",
        "            # Calculate sentiment statistics\n",
        "            stats = self._calculate_statistics(df)\n",
        "\n",
        "            # Save results\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            filename = f\"{stock_symbol}_reddit_all_{timestamp}.csv\"\n",
        "            df.to_csv(filename, index=False)\n",
        "            print(f\"\\nData saved to {filename}\")\n",
        "\n",
        "            return df, stats\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error searching Reddit: {str(e)}\")\n",
        "            return None, None\n",
        "\n",
        "    def _is_valid_stock_mention(self, text, symbol):\n",
        "        \"\"\"Check if stock mention is likely valid\"\"\"\n",
        "        # Convert text to uppercase for comparison\n",
        "        text = text.upper()\n",
        "        symbol = symbol.upper()\n",
        "\n",
        "        # Check for exact match with word boundaries\n",
        "        if f\" {symbol} \" in f\" {text} \":\n",
        "            # Additional checks to avoid false positives\n",
        "            common_words = [\"THE\", \"AND\", \"BUT\", \"FOR\", \"ARE\"]\n",
        "            if symbol in common_words:\n",
        "                return False\n",
        "\n",
        "            # Check for context\n",
        "            financial_terms = [\"STOCK\", \"SHARE\", \"PRICE\", \"MARKET\", \"TRADE\", \"BUY\", \"SELL\", \"$\"]\n",
        "            return any(term in text for term in financial_terms)\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _extract_post_data(self, submission, symbol, subreddit):\n",
        "        \"\"\"Extract relevant data from a submission\"\"\"\n",
        "        # Combine title and text for sentiment analysis\n",
        "        full_text = f\"{submission.title} {submission.selftext}\"\n",
        "        sentiment = self.analyze_text_sentiment(full_text)\n",
        "\n",
        "        return {\n",
        "            'id': submission.id,\n",
        "            'title': submission.title,\n",
        "            'text': submission.selftext,\n",
        "            'subreddit': subreddit,\n",
        "            'score': submission.score,\n",
        "            'upvote_ratio': submission.upvote_ratio,\n",
        "            'num_comments': submission.num_comments,\n",
        "            'created_utc': datetime.fromtimestamp(submission.created_utc),\n",
        "            'url': f\"https://reddit.com{submission.permalink}\",\n",
        "            'sentiment': sentiment['label'],\n",
        "            'sentiment_score': sentiment['score'],\n",
        "            'symbol': symbol\n",
        "        }\n",
        "\n",
        "    def _calculate_statistics(self, df):\n",
        "        \"\"\"Calculate various statistics about the posts\"\"\"\n",
        "        stats = {\n",
        "            'total_posts': len(df),\n",
        "            'unique_subreddits': df['subreddit'].nunique(),\n",
        "            'total_comments': df['num_comments'].sum(),\n",
        "            'avg_score': df['score'].mean(),\n",
        "            'sentiment_distribution': df['sentiment'].value_counts().to_dict(),\n",
        "            'top_subreddits': df['subreddit'].value_counts().head(5).to_dict(),\n",
        "            'high_impact_posts': df.nlargest(5, 'score')[\n",
        "                ['title', 'subreddit', 'sentiment', 'score', 'url']\n",
        "            ].to_dict('records')\n",
        "        }\n",
        "        return stats\n",
        "\n",
        "def print_analysis(symbol, df, stats):\n",
        "    \"\"\"Print analysis results in a readable format\"\"\"\n",
        "    print(f\"\\nAnalysis Results for {symbol}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    print(f\"\\nTotal Posts Found: {stats['total_posts']}\")\n",
        "    print(f\"Unique Subreddits: {stats['unique_subreddits']}\")\n",
        "    print(f\"Total Comments: {stats['total_comments']}\")\n",
        "    print(f\"Average Score: {stats['avg_score']:.2f}\")\n",
        "\n",
        "    print(\"\\nSentiment Distribution:\")\n",
        "    for sentiment, count in stats['sentiment_distribution'].items():\n",
        "        percentage = (count / stats['total_posts']) * 100\n",
        "        print(f\"{sentiment}: {count} posts ({percentage:.1f}%)\")\n",
        "\n",
        "    print(\"\\nTop Subreddits:\")\n",
        "    for subreddit, count in stats['top_subreddits'].items():\n",
        "        print(f\"{subreddit}: {count} posts\")\n",
        "\n",
        "    print(\"\\nMost Impactful Posts:\")\n",
        "    for post in stats['high_impact_posts']:\n",
        "        print(f\"\\nTitle: {post['title']}\")\n",
        "        print(f\"Subreddit: {post['subreddit']}\")\n",
        "        print(f\"Score: {post['score']}\")\n",
        "        print(f\"Sentiment: {post['sentiment']}\")\n",
        "        print(f\"URL: {post['url']}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize analyzer\n",
        "    analyzer = RedditStockAnalyzer(\n",
        "        client_id=\"YOUR_CLIENT_ID\",\n",
        "        client_secret=\"YOUR_CLIENT_SECRET\",\n",
        "        user_agent=\"StockMentionBot/1.0\"\n",
        "    )\n",
        "\n",
        "    # Search for stock mentions\n",
        "    stock_symbol = \"NVDA\"  # Change to any stock symbol\n",
        "    df, stats = analyzer.search_all_reddit(\n",
        "        stock_symbol=stock_symbol,\n",
        "        time_filter='week',  # Options: hour, day, week, month, year, all\n",
        "        limit=200\n",
        "    )\n",
        "\n",
        "    if df is not None and stats is not None:\n",
        "        print_analysis(stock_symbol, df, stats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04zFlCZZXSWK",
        "outputId": "c0da045c-ccec-45b0-e924-18cf62c9eaeb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching all Reddit for NVDA...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "200it [00:09, 21.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Searching r/wallstreetbets...\n",
            "\n",
            "Searching r/stocks...\n",
            "\n",
            "Searching r/investing...\n",
            "\n",
            "Searching r/stockmarket...\n",
            "\n",
            "Searching r/options...\n",
            "\n",
            "Searching r/pennystocks...\n",
            "\n",
            "Searching r/cryptocurrency...\n",
            "\n",
            "Searching r/finance...\n",
            "\n",
            "Searching r/business...\n",
            "\n",
            "Searching r/SecurityAnalysis...\n",
            "\n",
            "Searching r/algotrading...\n",
            "\n",
            "Data saved to NVDA_reddit_all_20241122_005735.csv\n",
            "\n",
            "Analysis Results for NVDA\n",
            "==================================================\n",
            "\n",
            "Total Posts Found: 79\n",
            "Unique Subreddits: 4\n",
            "Total Comments: 2211\n",
            "Average Score: 44.95\n",
            "\n",
            "Sentiment Distribution:\n",
            "neutral: 51 posts (64.6%)\n",
            "positive: 14 posts (17.7%)\n",
            "negative: 14 posts (17.7%)\n",
            "\n",
            "Top Subreddits:\n",
            "r/all: 73 posts\n",
            "wallstreetbets: 3 posts\n",
            "options: 2 posts\n",
            "stockmarket: 1 posts\n",
            "\n",
            "Most Impactful Posts:\n",
            "\n",
            "Title: Is RKLB the new NVDA or something? How is this kind of stock rise even possible??\n",
            "Subreddit: r/all\n",
            "Score: 723\n",
            "Sentiment: negative\n",
            "URL: https://reddit.com/r/wallstreetbets/comments/1gs0tva/is_rklb_the_new_nvda_or_something_how_is_this/\n",
            "\n",
            "Title: Day 12 of the 1k account challenge \n",
            "Subreddit: r/all\n",
            "Score: 683\n",
            "Sentiment: neutral\n",
            "URL: https://reddit.com/r/TheRaceTo10Million/comments/1gv8ltf/day_12_of_the_1k_account_challenge/\n",
            "\n",
            "Title: Thoughts On NVDA Earnings\n",
            "Subreddit: r/all\n",
            "Score: 132\n",
            "Sentiment: neutral\n",
            "URL: https://reddit.com/r/stocks/comments/1grrp1y/thoughts_on_nvda_earnings/\n",
            "\n",
            "Title: NVDA has Disclosed Ownership in New Company, Options play? \n",
            "Subreddit: r/all\n",
            "Score: 110\n",
            "Sentiment: neutral\n",
            "URL: https://reddit.com/r/wallstreetbets/comments/1gs1jhy/nvda_has_disclosed_ownership_in_new_company/\n",
            "\n",
            "Title: 85% Chance Up-Day Next Two Trading Sessions\n",
            "Subreddit: options\n",
            "Score: 103\n",
            "Sentiment: positive\n",
            "URL: https://reddit.com/r/options/comments/1gs8jin/85_chance_upday_next_two_trading_sessions/\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOEPqv5orh5UKb9Vgk1j2Mu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "032a243243734e7e9be9fcb72dad8cd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14c7c1087eaf490b91f244672c8a072c",
            "placeholder": "​",
            "style": "IPY_MODEL_37e30a3be4e34d589d195af403be5001",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "0bfe9537393346829f05c6feb7d26b2d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "126d9196623b4f8d88db587298a3aa5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d05b251602184a2e8d11ce4379ad58d2",
            "placeholder": "​",
            "style": "IPY_MODEL_93e4a07302fd4247b07216d94287b836",
            "value": " 2/2 [00:02&lt;00:00,  1.13it/s]"
          }
        },
        "14c7c1087eaf490b91f244672c8a072c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c07ea446a684a5d9f04b75e6381cb22": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "37e30a3be4e34d589d195af403be5001": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93e4a07302fd4247b07216d94287b836": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "953e0055f1fc4445b17b6d8e3b4dbc73": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_032a243243734e7e9be9fcb72dad8cd3",
              "IPY_MODEL_b5056af7e0e14cbc9ce9ca7808d1902f",
              "IPY_MODEL_126d9196623b4f8d88db587298a3aa5f"
            ],
            "layout": "IPY_MODEL_aa0ca269338c46d09b7fb3f278a29803"
          }
        },
        "aa0ca269338c46d09b7fb3f278a29803": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5056af7e0e14cbc9ce9ca7808d1902f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bfe9537393346829f05c6feb7d26b2d",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c07ea446a684a5d9f04b75e6381cb22",
            "value": 2
          }
        },
        "d05b251602184a2e8d11ce4379ad58d2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}